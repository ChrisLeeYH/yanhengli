---
title: "Human-LLM Framework for Analyzing Amplified Crisis Comments on YouTube"
authors:
- admin
- Da Wang
- Yuping Wang
author_notes:
- ""
- ""
- ""
date: "2025-12-10T00:00:00Z"

# Schedule page publish date (NOT publication's date).
publishDate: "2025-12-10T00:00:00Z"

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ["paper-conference"]

# Publication name and optional abbreviated publication name.
publication: "Submitted"
publication_short: "Submitted"

abstract: Online discourse during public crises plays a significant role in shaping public perception, resource mobilization, and emergency response. While extensive research has examined crisis discourse on textual platforms like $\mathbb{X}$ and Facebook, video-based platforms remain significantly under-explored despite their increasing prominence in modern media consumption. This paper addresses this gap by investigating comment amplification on YouTube through a novel Human-LLM framework. Our method deploys a multi-stage pipeline where LLMs work in tandem with human collaboration to analyze the ecosystem of crisis response. Specifically, the framework classifies content into "featuring" videos (dedicated solely to the crisis) and "curating" videos (aggregating various snippets), identifies key comment amplifiers, and characterizes the semantic and interaction nature of amplified content. To validate this approach, we analyzed two distinct crises-- the December 2024 US Government Shutdown (political strife) and the January 2025 California Wildfires (natural disaster). The application of our framework yielded critical insights into platform-specific dynamics, detecting high levels of polarization during both crises and identifying potential malicious algorithm injection during the political strife. This study contributes a robust, scalable methodological toolkit for multimedia crisis informatics, demonstrating how Human-LLM analysis can decode complex engagement patterns in video comment sections that were previously difficult to quantify at scale.

# Summary. An optional shortened abstract.
summary:

tags:
- Large Language Models
- Crisis
- Video Platform

featured: false

hugoblox:
  ids:
    doi: 

links:
  # - type: pdf
  #   url: https://dl.acm.org/doi/pdf/10.1145/3746252.3761274
  # - type: code
  #   url: ""
  # - type: dataset
  #   url: ""
  # - type: poster
  #   url: ""
  # - type: project
  #   url: ""
  # - type: slides
  #   url: https://www.slideshare.net/
  # - type: source
  #   url: ""
  # - type: video
  #   url: ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: 'Image created by chatGPT-5'
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

<!-- > [!NOTE]
> Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.

> [!NOTE]
> Create your slides in Markdown - click the *Slides* button to check out the example.

Add the publication's **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). -->

<!-- I co-examine the robustness of LLMs when used as automated peer reviewers.  
- Co-Designed an **aspect-guided multi-level perturbation framework**.  
- Co-Evaluated multiple LLMs on review tasks.  
- Found systematic **cognitive biases and stability gaps**.   -->